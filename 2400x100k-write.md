# Goal

How fast can we write 2400 100K files to blob storage.

# Results

The test results will be shown from fastest to slowest performance.

## TS Premium REST @ 16 x 8

**325 ms**

Using...

-   Typescript/Node
-   16 processes x 8 sockets each (connection pooling)
-   Premium Storage
-   REST

docker run -it -e "FILE_SIZE=102" -e "FILE_COUNT=2400" -e "MAX_SOCKETS=8" -e "PROCESSES=16" blob-cacher-node-perf
LOG_LEVEL is "info".
2019-03-04T15:47:46.347Z info: STORAGE_ACCOUNT is "pelasnehtbb".
2019-03-04T15:47:46.348Z info: STORAGE_CONTAINER is "input".
2019-03-04T15:47:46.348Z info: URL is "undefined".
2019-03-04T15:47:46.348Z info: STORAGE_KEY is "undefined"
2019-03-04T15:47:46.348Z info: STORAGE_SAS is "defined"
2019-03-04T15:47:46.348Z info: FILE_SIZE is "102" kb.
2019-03-04T15:47:46.348Z info: FILE_COUNT is "2400".
2019-03-04T15:47:46.348Z info: MAX_SOCKETS is "8".
2019-03-04T15:47:46.348Z info: PROCESSES is "16".
2019-03-04T15:47:47.600Z info: duration: 325 ms
2019-03-04T15:47:47.601Z info: count: 2400
2019-03-04T15:47:47.601Z info: wait: 266620 (91%)
2019-03-04T15:47:47.601Z info: dns: 4077 (1%)
2019-03-04T15:47:47.601Z info: tcp: 813 (0%)
2019-03-04T15:47:47.601Z info: firstByte: 20365 (7%)
2019-03-04T15:47:47.601Z info: download: 330 (0%)

## TS Premium REST @ 8 x 25

**435 ms**

Using...

-   Typescript/Node
-   8 processes x 25 sockets each (connection pooling)
-   Premium Storage
-   REST

This test proved that on higher throughput storage, we want more processes and fewer sockets. Likely this means that the service was getting "clogged" getting the data dispatched as fast as the service could take it.

docker run -it -e "FILE_SIZE=102" -e "FILE_COUNT=2400" -e "MAX_SOCKETS=25" -e "PROCESSES=8" blob-cacher-node-perf
LOG_LEVEL is "info".
2019-03-04T15:34:23.534Z info: STORAGE_ACCOUNT is "pelasnehtbb".
2019-03-04T15:34:23.535Z info: STORAGE_CONTAINER is "input".
2019-03-04T15:34:23.535Z info: URL is "undefined".
2019-03-04T15:34:23.536Z info: STORAGE_KEY is "undefined"
2019-03-04T15:34:23.536Z info: STORAGE_SAS is "defined"
2019-03-04T15:34:23.536Z info: FILE_SIZE is "102" kb.
2019-03-04T15:34:23.536Z info: FILE_COUNT is "2400".
2019-03-04T15:34:23.536Z info: MAX_SOCKETS is "25".
2019-03-04T15:34:23.536Z info: PROCESSES is "8".
2019-03-04T15:34:25.015Z info: duration: 435 ms
2019-03-04T15:34:25.015Z info: count: 2400
2019-03-04T15:34:25.015Z info: wait: 373907 (87%)
2019-03-04T15:34:25.015Z info: dns: 7558 (2%)
2019-03-04T15:34:25.016Z info: tcp: 1823 (0%)
2019-03-04T15:34:25.016Z info: firstByte: 47370 (11%)
2019-03-04T15:34:25.016Z info: download: 277 (0%)

## TS Standard REST @ 8 x 25

**741 ms**

Using...

-   Typescript/Node
-   8 processes x 25 sockets each (connection pooling)
-   Standard Storage
-   REST

docker run -it -e "FILE_SIZE=102" -e "FILE_COUNT=2400" -e "MAX_SOCKETS=25" -e "PROCESSES=8" blob-cacher-node-perf
LOG_LEVEL is "info".
2019-02-21T18:03:24.937Z info: STORAGE_ACCOUNT is "pelasneblobs".
2019-02-21T18:03:24.939Z info: STORAGE_CONTAINER is "input".
2019-02-21T18:03:24.939Z info: URL is "undefined".
2019-02-21T18:03:24.939Z info: STORAGE_KEY is "defined"
2019-02-21T18:03:24.939Z info: STORAGE_SAS is "defined"
2019-02-21T18:03:24.939Z info: FILE_SIZE is "102" kb.
2019-02-21T18:03:24.939Z info: FILE_COUNT is "2400".
2019-02-21T18:03:24.939Z info: MAX_SOCKETS is "25".
2019-02-21T18:03:24.939Z info: PROCESSES is "8".
2019-02-21T18:03:26.722Z info: duration: 741 ms
2019-02-21T18:03:26.722Z info: count: 2400
2019-02-21T18:03:26.722Z info: wait: 490972 (86%)
2019-02-21T18:03:26.722Z info: dns: 9155 (2%)
2019-02-21T18:03:26.722Z info: tcp: 2561 (0%)
2019-02-21T18:03:26.722Z info: firstByte: 65888 (12%)
2019-02-21T18:03:26.722Z info: download: 342 (0%)

## TS Standard REST @ 8 x 310

**1456 ms**

Using...

-   Typescript/Node
-   8 processes x 310 sockets each
-   Standard Storage
-   REST

With this test, I wanted to know if removing all wait for sockets was better than pooling a smaller number of sockets - it was not.

docker run -it -e "FILE_SIZE=102" -e "FILE_COUNT=2400" -e "MAX_SOCKETS=310" -e "PROCESSES=8" blob-cacher-node-perf
LOG_LEVEL is "info".
2019-02-21T18:01:26.695Z info: STORAGE_ACCOUNT is "pelasneblobs".
2019-02-21T18:01:26.696Z info: STORAGE_CONTAINER is "input".
2019-02-21T18:01:26.696Z info: URL is "undefined".
2019-02-21T18:01:26.696Z info: STORAGE_KEY is "defined"
2019-02-21T18:01:26.697Z info: STORAGE_SAS is "defined"
2019-02-21T18:01:26.697Z info: FILE_SIZE is "102" kb.
2019-02-21T18:01:26.697Z info: FILE_COUNT is "2400".
2019-02-21T18:01:26.697Z info: MAX_SOCKETS is "310".
2019-02-21T18:01:26.697Z info: PROCESSES is "8".
2019-02-21T18:01:29.240Z info: duration: 1456 ms
2019-02-21T18:01:29.240Z info: count: 2400
2019-02-21T18:01:29.240Z info: wait: 1539 (0%)
2019-02-21T18:01:29.240Z info: dns: 407240 (19%)
2019-02-21T18:01:29.240Z info: tcp: 679708 (31%)
2019-02-21T18:01:29.240Z info: firstByte: 1104449 (50%)
2019-02-21T18:01:29.240Z info: download: 327 (0%)

## DOTNET Standard SDK

**21788 ms**

Using...

-   .NET Core
-   Standard Storage
-   SDK

There is a lot of opportunity to improve this test, mostly it needs to:

-   be multi-threaded
-   ignore the MD5 checks

docker run -it -e "FILE_SIZE=102" -e "FILE_COUNT=2400" blob-cacher-dotnet-perf
FILE_SIZE is "102"
FILE_COUNT is "2400"
CONNECTION_STRING is "provided"
STORAGE_CONTAINER is "input"
21788 milliseconds

# Additional Testing

I would be interested to know if Golang would be better performing allowing for multiple threads rather than processes.
